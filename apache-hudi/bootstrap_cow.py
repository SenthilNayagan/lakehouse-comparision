"""Bootstrap - Loading data into Hudi Copy-on-Write (CoW) table.

Pre-requisites:
    Apache Hudi: 0.14.0
    Apache Spark: 3.4.1
"""

import os
from pyspark.sql import SparkSession, DataFrame


sourceDataPath = "../data"
sourceDataFile = os.path.join(sourceDataPath, "employees.csv")

basePath = "file:///tmp/emp_cow_hudi_table"

tableName = "emp_cow_hudi_table"
recordKey = "emp_id"  # Same as record keys. When record keys are not configured, bulk_insert will be chosen as the write operation.
partitionPath = "department"
precombineField = "last_performance_review_date"
writeOperation = "bulk_insert"

def get_spark_session():
    """Create and return the Spark session."""
    spark = (
        SparkSession.builder.master("local[2]").appName("Bootstrapping into Hudi CoW table.")
        .config(
            "spark.jars.packages", 
            "org.apache.hudi:hudi-spark3.4-bundle_2.12:0.14.0"
        )
        .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
        .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.hudi.catalog.HoodieCatalog")
        .config("spark.sql.extensions", "org.apache.spark.sql.hudi.HoodieSparkSessionExtension")
        .config("spark.kryo.registrator", "org.apache.spark.HoodieSparkKryoRegistrar")
        .getOrCreate()

    )
    return spark    


def read_data(spark: SparkSession, file: str):
    """Read data from the given path.

        Args:
            spark (SparkSession): Spark session.
            file (str): File to be read.

        Returns:
            DataFrame.
    """
    return spark.read.option("header", True).csv(file)

def write_into_hudi_table(spark: SparkSession, source_df: DataFrame, hudi_options: dict, base_path: str):
    """Write data into Hudi table.

        Args:
            spark (SparkSession): Spark session.
            source_df (DataFrame): Source Spark DataFrame to be written into Hudi table.
            hudi_options (dict): Hudi options.
            base_path: Hudi base path where Hudi table gets created.

        Returns:
            None.
    """
    # Write into Hudi table
    source_df.write.format("hudi"). \
        options(**hudi_options). \
        mode("overwrite"). \
        save(base_path)


if __name__ == '__main__':
    spark = get_spark_session()

    # Read source data
    source_df = read_data(spark, file=sourceDataFile)
    
    # Bootstrapping in to Hudi Copy-on-Write table.
    # When record key is not configured, bulk_insert will be chosen as the write operation.
    # If we have a workload without updates, we can also issue insert or bulk_insert operation which could be faster
    hudi_options = {
        "hoodie.table.name": tableName,
        "hoodie.datasource.write.recordkey.field": recordKey,  # It's optional. When not configured record key will be automatically generated by Hudi. 
        "hoodie.datasource.write.partitionpath.field": partitionPath,  # It's optional. 
        "hoodie.datasource.write.precombine.field": precombineField,
        "hoodie.datasource.write.operation": writeOperation,
        "hoodie.datasource.write.table.type": "COPY_ON_WRITE"  # Default to COPY_ON_WRITE.
    }

    write_into_hudi_table(
        spark, 
        source_df=source_df, 
        hudi_options=hudi_options, 
        base_path=basePath
    )
